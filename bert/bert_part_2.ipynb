{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasheill/test_repos/llm udemy/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 199\n",
      "===== Embedding Layer =====\n",
      "embeddings.word_embeddings.weight                       (30522, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "===== First Encoder =====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "===== Output Layer =====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "named_params = list(model.named_parameters())\n",
    "\n",
    "print(\"Number of layers:\", len(named_params))\n",
    "print('===== Embedding Layer =====')\n",
    "for p in named_params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n===== First Encoder =====\\n')\n",
    "for p in named_params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n===== Output Layer =====\\n')\n",
    "for p in named_params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pooler is a seperate linear and tanh activated layer that acts on the cls token's representation\n",
    "# this pooled_ouput is often used as a representation for the entire sentance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8254, 2319, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.encode('Sinan loves a beautiful day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokens through the model\n",
    "\n",
    "#1. Turn token_with_unkown_words into a tensor (will be size (8,))\n",
    "#2. Unsqueeze a first dimension to simulate batches, the resulting shape is (1,8)\n",
    "\n",
    "response = model(torch.tensor(tokenizer.encode('Sinan loves a beautiful day')).unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2327,  0.1515, -0.0448,  ..., -0.5192,  0.4195,  0.2948],\n",
       "         [ 0.3051, -0.6614,  0.2500,  ..., -0.9809,  0.2551,  0.2400],\n",
       "         [-0.3610, -0.8759,  0.4542,  ..., -1.1120,  0.1791,  0.0664],\n",
       "         ...,\n",
       "         [ 0.0689, -0.0364,  0.4940,  ..., -0.6558,  0.2227, -0.3868],\n",
       "         [-0.2657, -0.4257,  0.0056,  ...,  0.1352,  0.3596, -0.4585],\n",
       "         [ 0.6100,  0.0263, -0.2532,  ..., -0.0680, -0.3901, -0.3541]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding for each token, the first one being the [CLS] token\n",
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This layer is trained on top of the Embedding of the cls token. meant to represent the entire sentance as a whole\n",
    "\n",
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create the pooler ourselves we will grab the final represntation of the cls token\n",
    "\n",
    "CLS_embedding = response.last_hidden_state[:,0,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.pooler(CLS_embedding) == response.pooler_output).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Word Piece Tokenization\n",
    "\n",
    "## lets start by taking a look at the Bert tokenizer\n",
    "\n",
    "Let's use the `from_pretrained` method to grab the uncased bert-base tokenizer.\n",
    "\n",
    "A list of all availible modules can be found on their sike : https/huggingface.co/transformers/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of the vocabulary: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f'Lenght of the vocabulary: {len(tokenizer.vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 3722, 6251, 999, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"A simple sentence!\"\n",
    "tokens = tokenizer.encode(text) # get token ids per BERT-base's vocabulary\n",
    "print(tokens)\n",
    "# 101 is cls 102 is sep, and 999 is our exclamation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a simple sentence! [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode will re-construct the sentence with the added [CLS] and [SEP] tokens\n",
    "# we can also decode and get rid of the cls and sep tokens\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2026, 2767, 2409, 2033, 2055, 2023, 2465, 1998, 1045, 2293, 2009, 2061, 2521, 999, 2016, 2001, 2157, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "text = \"My friend told me about this class and I love it so far! she was right.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My friend told me about this class and I love it so far! she was right.. number of tokens: 20\n",
      "Token: 101, token string: [CLS]\n",
      "Token: 2026, token string: my\n",
      "Token: 2767, token string: friend\n",
      "Token: 2409, token string: told\n",
      "Token: 2033, token string: me\n",
      "Token: 2055, token string: about\n",
      "Token: 2023, token string: this\n",
      "Token: 2465, token string: class\n",
      "Token: 1998, token string: and\n",
      "Token: 1045, token string: i\n",
      "Token: 2293, token string: love\n",
      "Token: 2009, token string: it\n",
      "Token: 2061, token string: so\n",
      "Token: 2521, token string: far\n",
      "Token: 999, token string: !\n",
      "Token: 2016, token string: she\n",
      "Token: 2001, token string: was\n",
      "Token: 2157, token string: right\n",
      "Token: 1012, token string: .\n",
      "Token: 102, token string: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# a nicer printout of token ids and token strings\n",
    "def pretty_print_tokens(tokens, tokenizer):\n",
    "    print(f'Text: {text}. number of tokens: {len(tokens)}')\n",
    "    for t in tokens:\n",
    "            print(f'Token: {t}, token string: {tokenizer.decode([t])}')\n",
    "pretty_print_tokens(tokens, tokenizer)\n",
    "# in this case every word corresponds to a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sinan' in tokenizer.vocab\n",
    "\n",
    "#sinan is not in the vocabulary, so it is broken down into subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sinan will be broken down into `sin` and `##an`. The ## indicates it is part of another word and this is different from just `an`. I belive this would make sense as prefixes/suffixes can be encoded with a lot more meaning when given the `##` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My friend told me about this class and I love it so far! she was right.. number of tokens: 13\n",
      "Token: 101, token string: [CLS]\n",
      "Token: 8254, token string: sin\n",
      "Token: 2319, token string: ##an\n",
      "Token: 2003, token string: is\n",
      "Token: 2256, token string: our\n",
      "Token: 9450, token string: instructor\n",
      "Token: 2005, token string: for\n",
      "Token: 2023, token string: this\n",
      "Token: 12476, token string: awesome\n",
      "Token: 6342, token string: ##su\n",
      "Token: 10732, token string: ##ace\n",
      "Token: 2465, token string: class\n",
      "Token: 102, token string: [SEP]\n"
     ]
    }
   ],
   "source": [
    "text_with_unkown_words = \"Sinan is our instructor for this awesomesuace class\"\n",
    "tokens_with_unknown_words = tokenizer.encode(text_with_unkown_words)\n",
    "pretty_print_tokens(tokens_with_unknown_words, tokenizer)\n",
    "# awesomesauce becomes three words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8254, 2319, 2003, 2256, 9450, 2005, 2023, 12476, 6342, 10732, 2465, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "#encode plus gives us the token ids, attention mask (a sequence of ones or zeros if that \n",
    "#token should be included in attention calculation), useful for training time\n",
    "# also token_type_ids (sequence of zeros and ones) lets us know if we are passing one or two sequences into bert\n",
    "\n",
    "tokens= tokenizer.encode_plus(text_with_unkown_words)\n",
    "print(tokens)\n",
    "\n",
    "# calling tokenizer directly does the same thing as encode_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python is the 6th token\n",
    "python_pet = tokenizer.encode('I love my pet python')\n",
    "\n",
    "#python is the 6th token\n",
    "python_language = tokenizer.encode('I love coding in python')\n",
    "\n",
    "# we want to do a cosine similarity between the two python tokens\n",
    "\n",
    "#contextful embedding of 'python in 'ilove my pet python'\n",
    "python_pet_embedding=model(torch.tensor(python_pet).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "\n",
    "#look closely above we are taking the 6th token, which is python and then outputting to an numpy array\n",
    "\n",
    "python_language_embedding=model(torch.tensor(python_language).unsqueeze(0))[0][:,5,:].detach().numpy()\n",
    "# contextful embedding of snake in 'snake'\n",
    "snake_alone_embedding=model(torch.tensor(tokenizer.encode('snake')).unsqueeze(0))[0][:,1,:].detach().numpy()\n",
    "\n",
    "#contextful embedding of 'programming' in 'programming'\n",
    "programming_alone_embedding=model(torch.tensor(tokenizer.encode('programming')).unsqueeze(0))[0][:,1,:].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58434767]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets check the cosine similarities\n",
    "\n",
    "cosine_similarity(python_language_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6928657]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(python_pet_embedding, snake_alone_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5614741]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(python_language_embedding, programming_alone_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 the man embeddings of a BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "word_embeddings = context-free word embeddings\n",
    "position_embeddings = encodes word position\n",
    "token_type_embeddings == 0 or 1. Used to look up segment embeddings\n",
    "\"\"\"\n",
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2572, 8254, 2319,  102]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_phrase= \"I am Sinan\"\n",
    "\n",
    "tokenizer.encode(example_phrase, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "         [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],\n",
       "         [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],\n",
       "         [-0.0022, -0.0876,  0.0143,  ...,  0.0232, -0.0024, -0.0213],\n",
       "         [-0.0614, -0.0044, -0.0755,  ..., -0.0522, -0.0310, -0.0248],\n",
       "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0136, -0.0265, -0.0235,  ...,  0.0087,  0.0071,  0.0151],\n",
       "         [-0.0211,  0.0059, -0.0179,  ...,  0.0163,  0.0122,  0.0073],\n",
       "         [-0.0437, -0.0150,  0.0029,  ..., -0.0282,  0.0474, -0.0448],\n",
       "         [-0.0381, -0.0026,  0.0130,  ...,  0.0038, -0.0279, -0.0082],\n",
       "         [-0.0145, -0.0100,  0.0060,  ..., -0.0250,  0.0046, -0.0015]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.word_embeddings(tokenizer.encode('I am Matt', return_tensors='pt'))\n",
    "# first and last row of this one are the same as the last one because of the cls and sep token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(512, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7505e-02, -2.5631e-02, -3.6642e-02,  ...,  3.3437e-05,\n",
       "          6.8312e-04,  1.5441e-02],\n",
       "        [ 7.7580e-03,  2.2613e-03, -1.9444e-02,  ...,  2.8910e-02,\n",
       "          2.9753e-02, -5.3247e-03],\n",
       "        [-1.1287e-02, -1.9644e-03, -1.1573e-02,  ...,  1.4908e-02,\n",
       "          1.8741e-02, -7.3140e-03],\n",
       "        [-4.1949e-03, -1.1852e-02, -2.1180e-02,  ...,  2.2455e-02,\n",
       "          5.2826e-03, -1.9723e-03],\n",
       "        [-5.6087e-03, -1.0445e-02, -7.2288e-03,  ...,  2.0837e-02,\n",
       "          3.5402e-03,  4.7708e-03],\n",
       "        [-3.0871e-03, -1.8956e-02, -1.8930e-02,  ...,  7.4045e-03,\n",
       "          2.0183e-02,  3.4077e-03]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.position_embeddings(torch.LongTensor(range(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [-3.4026e-04,  5.3974e-01, -2.8805e-01,  ...,  7.5731e-01,\n",
       "           8.9008e-01,  1.6575e-01],\n",
       "         [-6.3496e-01,  1.9748e-01,  2.5116e-01,  ..., -4.0819e-02,\n",
       "           1.3468e+00, -6.9357e-01],\n",
       "         [ 2.8197e-01, -1.0037e+00,  3.5063e-01,  ...,  8.5378e-01,\n",
       "           3.9389e-01, -8.4527e-02],\n",
       "         [-7.3509e-01,  3.3429e-01, -8.3037e-01,  ..., -2.1545e-01,\n",
       "          -6.6517e-02, -2.6881e-02],\n",
       "         [-3.2507e-01, -3.1879e-01, -1.1632e-01,  ..., -3.9602e-01,\n",
       "           4.1120e-01, -7.7552e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings.LayerNorm(\n",
    "    model.embeddings.word_embeddings(tokenizer.encode(example_phrase, return_tensors='pt'))+\\\n",
    "    model.embeddings.position_embeddings(torch.LongTensor(range(6))) +\\\n",
    "    model.embeddings.token_type_embeddings(torch.LongTensor([0,0,0,0,0,0]))\n",
    ")\n",
    "#above is the same as below (We are just adding the embeddings and sending them thorugh\n",
    "\n",
    "model.embeddings(tokenizer.encode(example_phrase,return_tensors='pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
