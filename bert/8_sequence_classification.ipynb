{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast, # need this because we are using distil bert for classification (the fast version has bindings in c)\n",
    "    DataCollatorWithPadding, # The collater is responsible for generated batches of data\n",
    "    pipeline,\n",
    ")\n",
    "# we do distil bert for sequence classification for the speed\n",
    "from datasets import load_metric, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'listen O\\r\\n', b'to O\\r\\n', b'westbam B-artist\\r\\n', b'alumb O\\r\\n', b'allergic B-album\\r\\n', b'on O\\r\\n', b'google B-service\\r\\n', b'music I-service\\r\\n', b'PlayMusic\\r\\n', b'\\r\\n', b'add O\\r\\n', b'step B-entity_name\\r\\n', b'to I-entity_name\\r\\n', b'me I-entity_name\\r\\n', b'to O\\r\\n', b'the O\\r\\n', b'50 B-playlist\\r\\n', b'cl\\xc3\\xa1sicos I-playlist\\r\\n', b'playlist O\\r\\n', b'AddToPlaylist\\r\\n']\n"
     ]
    }
   ],
   "source": [
    "snips_file= open('../data/snips.train.txt', 'rb')\n",
    "snips_rows = snips_file.readlines()\n",
    "print(snips_rows[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['listen', 'to', 'westbam', 'alumb', 'allergic', 'on', 'google', 'music']\n",
      "['O', 'O', 'B-artist', 'O', 'B-album', 'O', 'B-service', 'I-service']\n",
      ",listentowestbamalumballergicongooglemusic\n",
      "PlayMusic\n"
     ]
    }
   ],
   "source": [
    "utterances = []\n",
    "tokenized_utterances = []\n",
    "labels_for_tokens = []\n",
    "sequence_labels = []\n",
    "\n",
    "utterance, tokenized_utterance, label_for_utterances = ',',[],[]\n",
    "for snip_row in snips_rows:\n",
    "    if len(snip_row)==2: #skip over rows with no data\n",
    "        continue\n",
    "    if ' ' not in snip_row.decode(): #we've hit a sequence label\n",
    "        sequence_labels.append(snip_row.decode().strip())\n",
    "        utterances.append(utterance.strip())\n",
    "        tokenized_utterances.append(tokenized_utterance)\n",
    "        labels_for_tokens.append(label_for_utterances)\n",
    "        utterance, tokenized_utterance, label_for_utterances = '',[],[]\n",
    "        continue\n",
    "    token, token_label = snip_row.decode().split(' ')\n",
    "    token_label = token_label.strip()\n",
    "    utterance +=f'{token}'\n",
    "    tokenized_utterance.append(token)\n",
    "    label_for_utterances.append(token_label)\n",
    "\n",
    "print(tokenized_utterances[0])\n",
    "print(labels_for_tokens[0])\n",
    "print(utterances[0])\n",
    "print(sequence_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sequence_labels = list(set(sequence_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique sequence labels\n"
     ]
    }
   ],
   "source": [
    "sequence_labels = [unique_sequence_labels.index(sequence_label) for sequence_label in sequence_labels]\n",
    "print(f'There are {len(unique_sequence_labels)} unique sequence labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 72 unique token labels\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "unique_token_labels = list(set(reduce(lambda x,y: x+y, labels_for_tokens)))\n",
    "labels_for_tokens = [[unique_token_labels.index(_) for _ in l] for l in labels_for_tokens]\n",
    "print(f'there are {len(unique_token_labels)} unique token labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_dataset = Dataset.from_dict(\n",
    "    dict(\n",
    "        utterance=utterances,\n",
    "        label=sequence_labels,\n",
    "        tokens=tokenized_utterances,\n",
    "        token_labels=labels_for_tokens\n",
    "    )\n",
    ")\n",
    "# now we can split u pthe dtaset\n",
    "snips_dataset = snips_dataset.train_test_split(test_size=0.2) #80 percent train, 20 percent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'bookconeyislandneighboringvaformarinaandi',\n",
       " 'label': 2,\n",
       " 'tokens': ['book',\n",
       "  'coney',\n",
       "  'island',\n",
       "  'neighboring',\n",
       "  'va',\n",
       "  'for',\n",
       "  'marina',\n",
       "  'and',\n",
       "  'i'],\n",
       " 'token_labels': [39, 37, 41, 31, 12, 39, 11, 9, 9]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snips_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 90.6kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.09MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 6.30MB/s]\n",
      "config.json: 100%|██████████| 483/483 [00:00<00:00, 4.36MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer =DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"utterance\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10467/10467 [00:00<00:00, 18039.86 examples/s]\n",
      "Map: 100%|██████████| 2617/2617 [00:00<00:00, 29854.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "seq_clf_tokenized_snips = snips_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utterance': 'bookconeyislandneighboringvaformarinaandi',\n",
       " 'label': 2,\n",
       " 'tokens': ['book',\n",
       "  'coney',\n",
       "  'island',\n",
       "  'neighboring',\n",
       "  'va',\n",
       "  'for',\n",
       "  'marina',\n",
       "  'and',\n",
       "  'i'],\n",
       " 'token_labels': [39, 37, 41, 31, 12, 39, 11, 9, 9],\n",
       " 'input_ids': [101,\n",
       "  2338,\n",
       "  8663,\n",
       "  3240,\n",
       "  2483,\n",
       "  3122,\n",
       "  2638,\n",
       "  18377,\n",
       "  12821,\n",
       "  2075,\n",
       "  3567,\n",
       "  14192,\n",
       "  27943,\n",
       "  5685,\n",
       "  2072,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_clf_tokenized_snips[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data collatore with padding creates batch of data. It also dynamically pads text to the\n",
    "length of the longest element in the batch, making them all the same length.\n",
    "It's possible to pad your text in the tokenizer with padding=True, dynamic padding is more efficient.\n",
    "\"\"\"\n",
    "\n",
    "data_collator= DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# attention mask is how we ignore attention scores for padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:04<00:00, 56.1MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sequence_clf_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels = len(unique_sequence_labels)\n",
    ")\n",
    "\n",
    "sequence_clf_model.config.id2label = {i:label for i, label in enumerate(unique_sequence_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric('accuracy')\n",
    "\n",
    "def custom_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
